# -*- coding: utf-8 -*-
"""DeepLearningProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m6BMEGt46Jwdz6_vyQJwUlRzOB0RnizA

# DEEP LEARNING PROJECT OF GROUP STUDENTS

## A project on breast cancer prediction using neural networks

Description : We are using the Wisconcin breast cancer data set to predict whether a woman has breast cancer or not. We have two classes "malign" and "benign" denoted as M and B respectively. We will be using pytorch which is an open-source machine learning library widely used for deep learning and artificial intelligence applications

## Importation of libraries
"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from torch.utils.data import DataLoader, TensorDataset

"""## Importation of dataset"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Input the name of your uploaded CSV file
df = pd.read_csv('new_dataset.csv')

"""## Exploratory Data Analysis"""

df.head()

print(df.shape) # (rows, columns)

print(df.describe())

print(df['diagnosis'].value_counts()) # count the number of benign and malignant cases

"""## Separation of Characteristics and Feature"""

X = df.drop(columns=['diagnosis'])
y = df['diagnosis'] #set the diagnosis variable as the target variable

"""## Convert string labels to numeric"""

le = LabelEncoder()
y = le.fit_transform(y)  # This will convert 'B' to 0 and 'M' to 1

"""## Separation of dataset into training set and test set"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""## Conversion of training and testing datasets from NumPy arrays (or other formats) into PyTorch tensors."""

X_train_tensor = torch.FloatTensor(X_train)
X_val_tensor = torch.FloatTensor(X_test)
y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)
y_val_tensor = torch.FloatTensor(y_test).view(-1, 1)

# Create DataLoader
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

"""## Creating the neural network architecture

This model is designed for binary classification tasks, with mechanisms for normalization and dropout to enhance training stability and reduce overfitting. The architecture consists of two hidden layers with 30 and 15 neurons, respectively, culminating in a single output neuron that produces a probability score.
"""

class BreastCancerModel(nn.Module):
    def __init__(self):
        super(BreastCancerModel, self).__init__()
        self.fc1 = nn.Linear(X_train.shape[1], 30)
        self.bn1 = nn.BatchNorm1d(30)
        self.dropout1 = nn.Dropout(0.3)  # 30% dropout
        self.fc2 = nn.Linear(30, 15)
        self.bn2 = nn.BatchNorm1d(15)
        self.dropout2 = nn.Dropout(0.3)  # 30% dropout
        self.fc3 = nn.Linear(15, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.bn1(self.fc1(x))
        x = torch.relu(x)
        x = self.dropout1(x)
        x = self.bn2(self.fc2(x))
        x = torch.relu(x)
        x = self.dropout2(x)
        x = self.sigmoid(self.fc3(x))
        return x

"""## Model Initialization, Loss Function and Optimizer

Here, we initialize the breast cancer prediction model, define the loss function for measuring performance, and set up the Adam optimizer for training the model.
"""

model = BreastCancerModel()
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0.01)

"""## Training the Model"""

# Lists to store loss and accuracy for plotting
train_losses = []
val_losses = []
train_accuracies = []
val_accuracies = []

# Training the model
num_epochs = 150
for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0
    correct = 0

    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        predicted = (outputs >= 0.5).float()
        correct += predicted.eq(batch_y).sum().item()

    # Average training loss and accuracy
    train_loss = epoch_loss / len(train_loader)
    train_losses.append(train_loss)
    train_accuracy = correct / len(train_loader.dataset)
    train_accuracies.append(train_accuracy)

    # Validate the model
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val_tensor)
        val_loss = criterion(val_outputs, y_val_tensor)
        val_losses.append(val_loss.item())
        val_predicted = (val_outputs >= 0.5).float()
        val_correct = val_predicted.eq(y_val_tensor).sum().item()
        val_accuracy = val_correct / len(y_val_tensor)
        val_accuracies.append(val_accuracy)

    # Print training and validation metrics for each epoch
    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy:.4f}')

"""This code segment trains the deep learning model for a specified number of epochs (150 in this case). During each epoch, it computes the loss and accuracy by performing forward and backward passes through the training data, updating the model's weights accordingly. The average training loss and accuracy for each epoch are stored in lists for later analysis and plotting.

## Loss Plot

A loss curve is a graphical representation of the loss value plotted against the number of training epochs (iterations). It typically shows two lines:

**Training Loss**: The training loss generally decreases steadily as training progresses. This trend indicates that the model is successfully learning from the training data.

**Validation Loss**: The validation loss initially decreases alongside the training loss, indicating that the model is generalizing well to the validation dataset.
After a certain number of epochs, however, the validation loss may start to increase while the training loss continues to decrease. This divergence is a key indicator of overfitting.
"""

# Loss Plot
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Training Loss', color='blue')
plt.plot(val_losses, label='Validation Loss', color='orange')
plt.title('Training vs Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

"""We can see that our model is not overfitting since both curves are decreasing and almost converge to the same value

## Accuracy plot
"""

# Accuracy Plot
plt.subplot(1, 2, 2)
plt.plot(train_accuracies, label='Training Accuracy', color='blue')
plt.plot(val_accuracies, label='Validation Accuracy', color='orange')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

"""## Evaluation of the model"""

from sklearn.metrics import precision_score, recall_score, f1_score
# Evaluate the model
model.eval()
with torch.no_grad():
    test_outputs = model(X_val_tensor)
    predicted = (test_outputs >= 0.5).float()
    accuracy = (predicted.eq(y_val_tensor).sum() / y_val_tensor.size(0)).item()
    # Calculate precision, recall, and F1 score
    precision = precision_score(y_val_tensor.numpy(), predicted.numpy())
    recall = recall_score(y_val_tensor.numpy(), predicted.numpy())
    f1 = f1_score(y_val_tensor.numpy(), predicted.numpy())

# Print test accuracy and additional metrics
print(f'Test Accuracy: {accuracy * 100:.2f}%')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')
# Print test accuracy
#print(f'Test Accuracy: {accuracy * 100:.2f}%')

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_val_tensor.numpy(), predicted.numpy())
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""## Interpretation of Metrics

Accuracy (99.00%):
This means that 99% of the predictions made by the model are correct. It's a strong indicator of overall model performance, especially in a binary classification scenario.

Precision (1.00):
A precision of 1.00 indicates that every positive prediction made by the model is correct. In the context of breast cancer diagnosis, this means that when the model predicts a tumor is malignant, it is indeed malignant all the time.

Recall (0.97):
A recall of 0.97 means that the model correctly identifies 97% of all actual positive cases (malignant tumors). This is crucial for medical applications, as missing a malignant tumor can have serious consequences.

F1 Score (0.98):
The F1 score is the harmonic mean of precision and recall. An F1 score of 0.98 suggests a good balance between precision and recall, indicating that the model is both precise in its positive predictions and effective at identifying positive cases.

Our model has an overall good performance based on the accuracy, F1 score, precision and recall

## Saving the Model

To export a trained PyTorch model, we save its state dictionary (which contains all the parameters of the model). The model will be saved as "breast_cancer_model.pth"
"""

# Save the model's state dictionary
torch.save(model.state_dict(), 'breast_cancer_model.pth')